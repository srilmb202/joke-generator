# joke-generator
This is based on LSTM algorithm, designed to predict and generate humorous text based on a dataset of jokes.
Traditional steps of an NLP application: <br />
* Tokenization and Sequence Creation
* Text Cleaning (stopwords removal,Stemming) and Preprocessing
* Model Training
* Model Validation

In LSTM implementation,it learns the patterns in the sequence of words that make up jokes. LSTM is well-suited for this task due to its ability to remember long-term dependencies, a crucial factor in generating relevant text.
Coming to t-SNE (t-Distributed Stochastic Neighbor Embedding) ,can gain insights in semantic relationships , upto what extent the model has learned.
